{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flekT6GFDN6m"
   },
   "source": [
    "#PREENCHA SEU NOME COMPLETO AQUI: \n",
    "\n",
    "### <span style=\"color:blue\">MBA em Ciência de Dados</span>\n",
    "### <span style=\"color:blue\">Redes Neurais e Arquiteturas Profundas</span>\n",
    "### <span style=\"color:blue\">Análise de Dados com Base em Processamento Massivo em Paralelo</span>\n",
    "\n",
    "### <span style=\"color:blue\">Prova Final</span>\n",
    "\n",
    "**Material Produzido por:**<br>\n",
    ">**Profa. Dra. Cristina Dutra de Aguiar**<br>\n",
    ">**Prof. Dr. Moacir A. Ponti**<br> \n",
    "\n",
    "**CEMEAI - ICMC/USP São Carlos**\n",
    "\n",
    "\n",
    "A prova final contém 1 questão, dividida em 3 itens. Por favor, procurem por Questão para encontrar a especificação da questão e por RESOLVER para encontrar a especificação do item a ser solucionado. Também é possível localizar a questão e os itens utilizando o menu de navegação. \n",
    "\n",
    "O notebook contém a constelação de fatos da BI Solutions que deve ser utilizada para responder à questão e também todas as `bibliotecas`, `bases de dados`, `inicializações`, `instalações`, `importações`, `geração de dataFrames`, `geração de visões temporárias` e `conversão dos tipos de dados` necessárias para a realização da questão.\n",
    "\n",
    "\n",
    "**INSTRUÇÕES**:<br>\n",
    "1) Você deve exportar esse notebook com sua solução para as questões da prova em formato .py e fazer upload no Moodle. Atenção: você não deve fazer upload de um arquivo notebook (.ipynb), mas sim um arquivo texto .py contendo os códigos python que utilizou para resolver as questões. O arquivo .py pode ser gerado através da opção:<br>\n",
    "File --> Download as --> Python (.py)\n",
    "disponível no Jupyter Notebook.\n",
    "\n",
    "ou\n",
    "File --> Download .py\n",
    "no Google Colab\n",
    "\n",
    "Caso não esteja utilizando o Jupyter, copie e cole seu código em um arquivo ASCII (Texto) salvando com a extensão .py\n",
    "\n",
    "2) Você deve salvar esse notebook com sua solução para as questões da prova em formato .pdf e fazer upload no Moodle\n",
    "\n",
    "3) Os arquivos devem ser nomeados com seu nome e sobrenome, sem espaços. Exemplo: moacirponti.py e moacirponti.pdf\n",
    "\n",
    "4) É OBRIGATÓRIO conter no cabeçalho (início) do arquivo um comentário / texto com o seu nome completo\n",
    "\n",
    "\n",
    "**Desejamos uma boa prova!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3o3dN_WLQcyD"
   },
   "source": [
    "#1 Constelação de Fatos da BI Solutions\n",
    "\n",
    "A aplicação de *data warehousing* da BI Solutions utiliza como base uma contelação de fatos, conforme descrita a seguir.\n",
    "\n",
    "**Tabelas de dimensão**\n",
    "\n",
    "- data (dataPK, dataCompleta, dataDia, dataMes, dataBimestre, dataTrimestre, dataSemestre, dataAno)\n",
    "- funcionario (funcPK, funcMatricula, funcNome, funcSexo, funcDataNascimento, funcDiaNascimento, funcMesNascimento, funcAnoNascimento, funcCidade, funcEstadoNome, funcEstadoSigla, funcRegiaoNome, funcRegiaoSigla, funcPaisNome, funcPaisSigla)\n",
    "- equipe (equipePK, equipeNome, filialNome, filialCidade, filialEstadoNome, filialEstadoSigla, filialRegiaoNome, filialRegiaoSigla, filialPaisNome, filialPaisSigla)\n",
    "- cargo (cargoPK, cargoNome, cargoRegimeTrabalho, cargoEscolaridadeMinima, cargoNivel)\n",
    "- cliente (clientePK, clienteNomeFantasia, clienteSetor, clienteCidade, clienteEstadoNome, clienteEstadoSigla, clienteRegiaoNome, clienteRegiaoSigla, clientePaisNome, clientePaisSigla)\n",
    "\n",
    "**Tabelas de fatos**\n",
    "- pagamento (dataPK, funcPK, equipePK, cargoPK, salario, quantidadeLancamentos)\n",
    "- negociacao (dataPK, equipePK, clientePK, receita, quantidadeNegociacoes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGeh8KdXwVCQ"
   },
   "source": [
    "#2 Configurações \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWX2HVtNSw8w"
   },
   "source": [
    "## 2.1 Obtenção dos Dados da BI Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3e0Eao1K0EYG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "#instalando o módulo wget\n",
    "%%capture\n",
    "!pip install -q wget\n",
    "!mkdir data\n",
    "\n",
    "#baixando os dados das tabelas de dimensão e das tabelas de fatos\n",
    "import wget\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/data.csv\"\n",
    "wget.download(url, \"data/data.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/funcionario.csv\"\n",
    "wget.download(url, \"data/funcionario.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/equipe.csv\"\n",
    "wget.download(url, \"data/equipe.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/cargo.csv\"\n",
    "wget.download(url, \"data/cargo.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/cliente.csv\"\n",
    "wget.download(url, \"data/cliente.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/pagamento.csv\"\n",
    "wget.download(url, \"data/pagamento.csv\")\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/cdaciferri/DataMartBISolutions/main/negociacao.csv\"\n",
    "wget.download(url, \"data/negociacao.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO16-7-jOioq"
   },
   "source": [
    "## 2.2 Instalações e Inicializações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gFfZ3QoxuV6q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "#instalando Java Runtime Environment (JRE) versão 8\n",
    "%%capture\n",
    "!apt-get remove openjdk*\n",
    "!apt-get update --fix-missing\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hpSR-ffXuZS3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "#baixando Apache Spark versão 3.0.0\n",
    "%%capture\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
    "!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XC4whsGcuiEF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#configurando a variável de ambiente JAVA_HOME\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "#configurando a variável de ambiente SPARK_HOME\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZMmfchBEuky0"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#instalando o pacote findspark\n",
    "!pip install -q findspark==1.4.2\n",
    "#instalando o pacote pyspark\n",
    "!pip install -q pyspark==3.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_8fiSPVwqAO"
   },
   "source": [
    "## 2.3 Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NXls3bfoglKW"
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-819bfec0404e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pyspark-notebook\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[*]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lib\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"py4j-*.zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         raise Exception(\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[1;34m\"Unable to find py4j, your SPARK_HOME may not be configured correctly\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         )\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import round, desc\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcSO2MjXC23E"
   },
   "source": [
    "## 2.4 Geração dos DataFrames em Pandas da BI Solutions\n",
    "\n",
    "Nesta seção são gerados os DataFrames em Pandas. Atenção aos nomes desses DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a0nRg8NC83e"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyQlExKoDBrb"
   },
   "outputs": [],
   "source": [
    "cargoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv')\n",
    "clientePandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv')\n",
    "dataPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv')\n",
    "equipePandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv')\n",
    "funcionarioPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv')\n",
    "negociacaoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv')\n",
    "pagamentoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qL9SiR_pQE2"
   },
   "source": [
    "## 2.5 Geração dos DataFrames em Spark da BI Solutions\n",
    "\n",
    "Nesta seção são gerados dos DataFrames em Spark. Atenção aos nomes desses DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "id": "FNR-3dV6oYk4"
   },
   "outputs": [],
   "source": [
    "#criando os DataFrames em Spark \n",
    "cargo = spark.read.csv(path=\"data/cargo.csv\", header=True, sep=\",\")\n",
    "cliente = spark.read.csv(path=\"data/cliente.csv\", header=True, sep=\",\")\n",
    "data = spark.read.csv(path=\"data/data.csv\", header=True, sep=\",\")\n",
    "equipe = spark.read.csv(path=\"data/equipe.csv\", header=True, sep=\",\")\n",
    "funcionario = spark.read.csv(path=\"data/funcionario.csv\", header=True, sep=\",\")\n",
    "negociacao = spark.read.csv(path=\"data/negociacao.csv\", header=True, sep=\",\")\n",
    "pagamento = spark.read.csv(path=\"data/pagamento.csv\", header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmCV6Mur__z6"
   },
   "outputs": [],
   "source": [
    "#convertendo os dados necessários para o tipo de dado inteiro\n",
    "colunas_cargo = [\"cargoPK\"]\n",
    "colunas_cliente = [\"clientePK\"]\n",
    "colunas_data = [\"dataPK\", \"dataDia\", \"dataMes\", \"dataBimestre\", \"dataTrimestre\", \"dataSemestre\", \"dataAno\"]\n",
    "colunas_equipe = [\"equipePK\"]\n",
    "colunas_funcionario = [\"funcPK\", \"funcDiaNascimento\", \"funcMesNascimento\", \"funcAnoNascimento\"]\n",
    "colunas_negociacao = [\"equipePK\", \"clientePK\", \"dataPK\", \"quantidadeNegociacoes\"]\n",
    "colunas_pagamento = [\"funcPK\", \"equipePK\", \"dataPK\", \"cargoPK\", \"quantidadeLancamentos\"]\n",
    "\n",
    "for coluna in colunas_cargo:\n",
    "  cargo = cargo.withColumn(coluna, cargo[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_cliente:\n",
    "  cliente = cliente.withColumn(coluna, cliente[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_data:\n",
    "  data = data.withColumn(coluna, data[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_equipe:\n",
    "  equipe = equipe.withColumn(coluna, equipe[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_funcionario:\n",
    "  funcionario = funcionario.withColumn(coluna, funcionario[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_negociacao:\n",
    "  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(IntegerType()))\n",
    "\n",
    "for coluna in colunas_pagamento:\n",
    "  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBcQ7Ep7AWqN"
   },
   "outputs": [],
   "source": [
    "#convertendo os dados necessários para o tipo de dado float\n",
    "colunas_negociacao = [\"receita\"]\n",
    "colunas_pagamento = [\"salario\"]\n",
    "\n",
    "for coluna in colunas_negociacao:\n",
    "  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(FloatType()))\n",
    "\n",
    "for coluna in colunas_pagamento:\n",
    "  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJsqRI3TwsjS"
   },
   "outputs": [],
   "source": [
    "#criando as visões temporárias \n",
    "cargo.createOrReplaceTempView(\"cargo\")\n",
    "cliente.createOrReplaceTempView(\"cliente\")\n",
    "data.createOrReplaceTempView(\"data\")\n",
    "equipe.createOrReplaceTempView(\"equipe\")\n",
    "funcionario.createOrReplaceTempView(\"funcionario\")\n",
    "negociacao.createOrReplaceTempView(\"negociacao\")\n",
    "pagamento.createOrReplaceTempView(\"pagamento\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNAPADPMPProvaFinalNobebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
